{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c61cb82d-ce8f-4fa8-9253-5f1bf930bded",
   "metadata": {},
   "source": [
    "<p style=\"float:right\"> <img src=\"assets/orange.png\" alt=\"Orange logo\" width=\"40\" /> <img src=\"assets/ulb.jpg\" alt=\"ULB logo\" width=\"40\" /> <img src=\"assets/mlg.png\" alt=\"MLG logo\" width=\"160\" /> <img src=\"assets/innoviris.jpg\" alt=\"Innoviris logo\" width=\"200\" /></p>\n",
    "\n",
    "**_Notebook for the project Machu-Picchu written by Th√©o Verhelst_**<br/>\n",
    "_Supervisors at Orange: Denis Mercier, Jeevan Shrestha_<br/>\n",
    "_Academic supervision: Gianluca Bontempi (ULB MLG)_\n",
    "# Simulating uplift modeling with normal distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb7deeb-059e-41f5-862a-1de82e19d6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from numpy.polynomial import polynomial\n",
    "import pandas as pd\n",
    "from scipy import stats as st\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.autonotebook import trange, tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "from functions.eval_measures import cf_profit_curve\n",
    "from functions.simulation_functions import simulate_uplift_norm\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"mathtext.fontset\"] = \"dejavuserif\"\n",
    "matplotlib.rc('text.latex', preamble=r'\\usepackage{amsmath}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5f1450-5583-483f-99ad-7d910b83d44e",
   "metadata": {},
   "source": [
    "Here, the data generating process is\n",
    "\\begin{align}\n",
    "    X&\\sim\\mathcal N(0, I) \\\\\n",
    "    \\varepsilon&\\sim\\mathcal N(0, 1) \\\\\n",
    "    T&\\sim\\mathrm{Bern}(p) \\\\\n",
    "    Y_t &= \\mathbb I[\\lambda_t^TX+\\varepsilon\\ge\\tau_t]  \\quad\\text{for }t=0,1\\\\\n",
    "    Y &= Y_{\\mathbb I[T=1]}.\n",
    "\\end{align}\n",
    "We know that $\\lambda_t^TX\\sim\\mathcal N(0, \\lambda_t^T\\lambda_t)$.\n",
    "Then the conditional distribution of $Y_t$ can be computed as\n",
    "\\begin{align}\n",
    "    P(Y_t=1\\mid X=x) &= P(\\lambda_t^Tx+\\varepsilon>\\tau_t)=P(\\varepsilon\\ge\\tau_t-\\lambda_t^Tx)\\\\\n",
    "    &=1 - F_{\\varepsilon}(\\tau_t-\\lambda_t^Tx)=F_{\\varepsilon}(\\lambda_t^Tx-\\tau_t).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2603080a-2526-4a04-aa48-6ba5e3a24e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_info_rate(S_0, S_1):\n",
    "    H_Y_0 = st.entropy([np.mean(S_0), 1 - np.mean(S_0)])\n",
    "    H_Y_1 = st.entropy([np.mean(S_1), 1 - np.mean(S_1)])\n",
    "    H_Y_0_X = np.mean(st.entropy(np.vstack((S_0, 1 - S_0))))\n",
    "    H_Y_1_X = np.mean(st.entropy(np.vstack((S_1, 1 - S_1))))\n",
    "    I_0 = 1 - H_Y_0_X / H_Y_0\n",
    "    I_1 = 1 - H_Y_1_X / H_Y_1\n",
    "    return I_0, I_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e7fcf3-f3b6-4453-8b22-98512757100d",
   "metadata": {},
   "source": [
    "### Run the simulation once\n",
    "As a preliminary step, we run once the simulation and verify the distribution of the generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41c476f-ab76-416d-a3aa-7a8bb0488e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = 10000\n",
    "N_test = 100000\n",
    "N = N_train + N_test\n",
    "n = 100\n",
    "\n",
    "# Generate the parameters\n",
    "mu = np.ones(n)\n",
    "Sigma = np.eye(n)\n",
    "lambda_0 = st.norm.rvs(loc=0, scale=15, size=n) / n\n",
    "lambda_1 = st.norm.rvs(loc=0, scale=15, size=n) / n\n",
    "\n",
    "# Generate the data\n",
    "X = st.multivariate_normal.rvs(mean=mu, cov=Sigma, size=N)\n",
    "noise = st.norm.rvs(size=N)\n",
    "Q_0 = np.dot(X, lambda_0) + noise\n",
    "Q_1 = np.dot(X, lambda_1) + noise\n",
    "\n",
    "eta_0, eta_1 = 1.12, 0.87\n",
    "Y_0 = Q_0 > eta_0\n",
    "Y_1 = Q_1 > eta_1\n",
    "a = np.mean(~Y_0 & ~Y_1)\n",
    "b  = np.mean( Y_0 & ~Y_1)\n",
    "c = np.mean(~Y_0 &  Y_1)\n",
    "d = np.mean( Y_0 &  Y_1)\n",
    "\n",
    "print(a, b, c, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295f85aa-f536-4089-bae2-9939ddf6c4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.dot(X, lambda_0) + noise, np.dot(X, lambda_1) + noise, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411d223f-37f9-474f-ab97-adbf20b9a01d",
   "metadata": {},
   "source": [
    "### Multiple runs with fixed parameters\n",
    "Here, we use a fixed value for the parameters, except that lambda_0 and lambda_1 are multiplied by a parameter ranging from 1e-2 to 1e1.3 on a logarithmic scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b1deea-f141-400d-8d4b-ee4dbf1307d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = 1000\n",
    "N_test = 10000\n",
    "N = N_train + N_test\n",
    "n = 10\n",
    "p = 0.1\n",
    "\n",
    "seed = 444\n",
    "#seed = 333\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "# Generate the parameters\n",
    "mu = np.zeros(n)\n",
    "Sigma = np.eye(n)\n",
    "lambda_0 = st.norm.rvs(loc=1.2, scale=1, size=n, random_state=rng)\n",
    "lambda_1 = st.norm.rvs(loc=1, scale=1, size=n, random_state=rng)\n",
    "print(\"lambda_0 = \", \", \".join(\"{:.2f}\".format(v) for v in lambda_0))\n",
    "print(\"lambda_1 = \", \", \".join(\"{:.2f}\".format(v) for v in lambda_1))\n",
    "\n",
    "# Generate the data\n",
    "X = st.multivariate_normal.rvs(mean=mu, cov=Sigma, size=N, random_state=rng)\n",
    "noise = st.logistic.rvs(size=N, random_state=rng)\n",
    "\n",
    "Q_0 = np.dot(X, lambda_0) + noise\n",
    "Q_1 = np.dot(X, lambda_1) + noise\n",
    "eta_0, eta_1 = 1.12, 0.87\n",
    "Y_0 = Q_0 > eta_0\n",
    "Y_1 = Q_1 > eta_1\n",
    "a = np.mean(~Y_0 & ~Y_1)\n",
    "b  = np.mean( Y_0 & ~Y_1)\n",
    "c = np.mean(~Y_0 &  Y_1)\n",
    "d = np.mean( Y_0 &  Y_1)\n",
    "\n",
    "print(\"eta_0 = {:.2f}, eta_1 = {:.2f}\".format(eta_0, eta_1))\n",
    "print(\"mu = {:.2f}, {:.2f}, {:.2f}, {:.2f}\".format(a, b, c, d))\n",
    "\n",
    "results = []\n",
    "\n",
    "for scale in tqdm(np.logspace(-2, 1.3, num=100)):\n",
    "    l_0 = lambda_0 * scale\n",
    "    l_1 = lambda_1 * scale\n",
    "    \n",
    "    # Sometimes the probability of Y_1=y is too low and we have\n",
    "    # zero sample in this class. Let's try again in this case.\n",
    "    while True:\n",
    "        try:\n",
    "            auuc_u, auuc_p = simulate_uplift_norm(N_train, X, noise, l_0, l_1, eta_0, eta_1, p, 1000, rng)\n",
    "            break\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    # Use more data to compute the information rate\n",
    "    N_p = N * 10\n",
    "    X_p = st.multivariate_normal.rvs(mean=mu, cov=Sigma, size=N_p, random_state=rng)\n",
    "    S_0_p = st.logistic.cdf(np.dot(X_p, l_0))\n",
    "    S_1_p = st.logistic.cdf(np.dot(X_p, l_1))\n",
    "    \n",
    "    I_0, I_1 = compute_info_rate(S_0_p, S_1_p)\n",
    "    results.append({\n",
    "        \"scale\": scale,\n",
    "        \"auuc_u\": auuc_u,\n",
    "        \"auuc_p\": auuc_p,\n",
    "        \"I_0\": I_0,\n",
    "        \"I_1\": I_1\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce00a174-fd9b-45a9-85d1-a911d8916883",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (5.9, 3.15)\n",
    "\n",
    "df = pd.DataFrame.from_records(results)\n",
    "plt.plot(df.I_0, df.auuc_u * 100, label=\"Uplift approach\", color=\"black\", linestyle=\"solid\")\n",
    "plt.plot(df.I_0, df.auuc_p * 100, label=\"Predictive approach\", color=\"black\", linestyle=\"dashed\")\n",
    "plt.ylabel(\"AUPC (%)\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(r\"$I({\\bf x}; {\\bf y}_0)\\;/\\;H({\\bf y}_0)$\")\n",
    "plt.legend()\n",
    "\n",
    "#plt.savefig(\"pdf/simulation_norm.pdf\", bbox_inches=\"tight\", dpi=240)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e81072-676e-43be-a019-a1b3eede60e0",
   "metadata": {},
   "source": [
    "### Multiple runs with random parameters\n",
    "Here, we randomize each parameter each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96550841-5ac1-40ca-bf47-10d7458c6f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = 1000\n",
    "N_test = 10000\n",
    "scale_space_size = 100\n",
    "n = 10\n",
    "p = 0.04\n",
    "n_repeats = 100\n",
    "\n",
    "seed = 444\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "def run_normal_dis(i, N_train, N_test, scale_space_size, n, p, rng):\n",
    "    rng = None\n",
    "    # Generate the parameters\n",
    "    N = N_train + N_test\n",
    "    mu = np.zeros(n)\n",
    "    Sigma = np.eye(n)\n",
    "    lambda_0 = st.norm.rvs(loc=1.2, scale=1, size=n, random_state=rng)\n",
    "    lambda_1 = st.norm.rvs(loc=1, scale=1, size=n, random_state=rng)\n",
    "\n",
    "    # Generate the data\n",
    "    X = st.multivariate_normal.rvs(mean=mu, cov=Sigma, size=N, random_state=rng)\n",
    "    noise = st.logistic.rvs(size=N, random_state=rng)\n",
    "\n",
    "    # Find the thresholds that closely match the desired counterfactual distribution\n",
    "    alpha, beta, gamma, delta = 0.4, 0.2, 0.2, 0.2\n",
    "    #alpha, beta, gamma, delta = 0.7, 0.1, 0.1, 0.1\n",
    "    Q_0 = np.dot(X, lambda_0) + noise\n",
    "    Q_1 = np.dot(X, lambda_1) + noise\n",
    "    eta_0, eta_1 = 1.12, 0.87\n",
    "    Y_0 = Q_0 > eta_0\n",
    "    Y_1 = Q_1 > eta_1\n",
    "    a = np.mean(~Y_0 & ~Y_1)\n",
    "    b  = np.mean( Y_0 & ~Y_1)\n",
    "    c = np.mean(~Y_0 &  Y_1)\n",
    "    d = np.mean( Y_0 &  Y_1)\n",
    "\n",
    "    results = []\n",
    "    for scale in np.logspace(-2, 1.3, num=scale_space_size):\n",
    "        l_0 = lambda_0 * scale\n",
    "        l_1 = lambda_1 * scale\n",
    "\n",
    "        # Sometimes the probability of Y_1=y is too low and we have\n",
    "        # zero sample in this class. Let's try again in this case.\n",
    "        T = 0\n",
    "        while True:\n",
    "            try:\n",
    "                auuc_u, auuc_p = simulate_uplift_norm(N_train, X, noise, l_0, l_1, eta_0, eta_1, p, 1000, rng)\n",
    "                break\n",
    "            except ValueError:\n",
    "                T += 1\n",
    "                if T >= 1000:\n",
    "                    break\n",
    "                continue\n",
    "        if T >= 1000:\n",
    "            continue\n",
    "\n",
    "        # Use more data to compute the information rate\n",
    "        N_p = N * 10\n",
    "        X_p = st.multivariate_normal.rvs(mean=mu, cov=Sigma, size=N_p, random_state=rng)\n",
    "        S_0_p = st.logistic.cdf(np.dot(X_p, l_0))\n",
    "        S_1_p = st.logistic.cdf(np.dot(X_p, l_1))\n",
    "\n",
    "        I_0, I_1 = compute_info_rate(S_0_p, S_1_p)\n",
    "        results.append({\n",
    "            \"i\": i,\n",
    "            \"scale\": scale,\n",
    "            \"auuc_u\": auuc_u,\n",
    "            \"auuc_p\": auuc_p,\n",
    "            \"I_0\": I_0,\n",
    "            \"I_1\": I_1\n",
    "        })\n",
    "    return results\n",
    "\n",
    "results = Parallel(n_jobs=4)(\n",
    "    delayed(run_normal_dis)(\n",
    "        i, N_train, N_test, scale_space_size, n, p, rng\n",
    "    ) for i in trange(n_repeats)\n",
    ")\n",
    "results = sum(results, [])\n",
    "\n",
    "stats = pd.DataFrame.from_records(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17f2eed-cb25-4814-9062-d7307e2ec67a",
   "metadata": {},
   "source": [
    "These two functions are used to create the plot below. The AUPC is plotted as a function of the mutual information by fitting a 4th-degree polynomial. Then, we use a moving Gaussian kernel to estimate locally the standard deviation of the AUPC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6136ec-33d0-4d56-a7e3-7eed41b008f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weigthed_sample_std(x, x_bar, weights):\n",
    "    weights /= weights.sum()\n",
    "    squared_error = np.dot((x - x_bar)**2, weights)\n",
    "    N = x.shape[0]\n",
    "    return np.sqrt(squared_error * N / (N - 1))\n",
    "\n",
    "def confidence_curves(x, y, fitted_x, fitted_y, bw, alpha):\n",
    "    cb_min = []\n",
    "    cb_max = []\n",
    "    N = len(fitted_x)\n",
    "    for i in range(N):\n",
    "        # Use a gaussian window on the data points\n",
    "        weights = st.norm.pdf(x, loc=fitted_x[i], scale=bw)\n",
    "        # Fix the location to the predicted value\n",
    "        mean = fitted_y[i]\n",
    "        sample_var = weigthed_sample_std(y, mean, weights)\n",
    "        #_, sample_var = st.norm.fit(y_w, loc=mean)\n",
    "        z = (1 - alpha) / 2\n",
    "        cb_min.append(st.norm.ppf(z, loc=mean, scale=sample_var))\n",
    "        cb_max.append(st.norm.ppf(1 - z, loc=mean, scale=sample_var))\n",
    "    return cb_min, cb_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f949fd-f1f7-4191-89c3-181c8b57e77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = stats.I_0\n",
    "y_u = stats.auuc_u * 100\n",
    "y_p = stats.auuc_p * 100\n",
    "P_u = polynomial.Polynomial.fit(x=x, y=y_u, deg=4)\n",
    "P_p = polynomial.Polynomial.fit(x=x, y=y_p, deg=4)\n",
    "space_u = P_u.linspace()\n",
    "space_p = P_p.linspace()\n",
    "cb_min_u, cb_max_u = confidence_curves(x, y_u, space_u[0], space_u[1], 0.01, 0.95)\n",
    "cb_min_p, cb_max_p = confidence_curves(x, y_p, space_p[0], space_p[1], 0.01, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd224d26-5e32-481b-9145-0f0bee35e5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (5.9, 3.15)\n",
    "cmap = {\n",
    "    \"u\": (31, 119, 180),\n",
    "    \"p\": (255,127,14),\n",
    "    \"e\": (148,103,189)\n",
    "}\n",
    "def rgb_to_hex(rgb):\n",
    "    return tuple(c/255 for c in rgb)\n",
    "\n",
    "#plt.scatter(x, y_u, label=\"Uplift approach\", color=rgb_to_hex(cmap[\"u\"]), alpha=0.02)\n",
    "plt.plot(space_u[0], space_u[1], color=rgb_to_hex(cmap[\"u\"]), label=\"Uplift\")\n",
    "plt.fill_between(space_u[0], cb_min_u, cb_max_u, color=rgb_to_hex(cmap[\"u\"]), alpha=0.2)\n",
    "#plt.scatter(x, y_p, label=\"Predictive approach\", color=rgb_to_hex(cmap[\"p\"]), alpha=0.02)\n",
    "plt.plot(space_p[0], space_p[1], color=rgb_to_hex(cmap[\"p\"]), label=\"Predictive\")\n",
    "plt.fill_between(space_p[0], cb_min_p, cb_max_p, color=rgb_to_hex(cmap[\"p\"]), alpha=0.2)\n",
    "plt.ylabel(\"AUPC (%)\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(r\"$I({\\bf x}; {\\bf y}_0)\\;/\\;H({\\bf y}_0)$\")\n",
    "plt.legend(title=\"Approach\", loc=\"upper left\")\n",
    "\n",
    "#plt.savefig(\"pdf/simulation_norm_repeated.pdf\", bbox_inches=\"tight\", dpi=240)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc3bd7f-df9c-4d1e-8ced-2b2cd28f5b37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "r-cpu.4-1.m89",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/r-cpu.4-1:m89"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
